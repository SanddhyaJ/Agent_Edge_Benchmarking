{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215e2191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164ec9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: mas_safety_gpt4o_mmlu_safety\n",
      "Parsed 1533 responses from mas_safety_gpt4o_mmlu_safety\n",
      "Processing folder: mas_metacognition_gpt4o_medcalc_metacognition\n",
      "Parsed 411 responses from mas_metacognition_gpt4o_medcalc_metacognition\n",
      "Processing folder: mas_metacognition_gpt4o_pubmedqa_metacognition\n",
      "Parsed 1000 responses from mas_metacognition_gpt4o_pubmedqa_metacognition\n",
      "Processing folder: mas_metacognition_gpt4o_medbullets_metacognition\n",
      "Parsed 308 responses from mas_metacognition_gpt4o_medbullets_metacognition\n",
      "Processing folder: mas_safety_gpt4o_mmlupro_safety\n",
      "Parsed 1101 responses from mas_safety_gpt4o_mmlupro_safety\n",
      "Processing folder: mas_safety_gpt4o_casehold_safety\n",
      "Parsed 403 responses from mas_safety_gpt4o_casehold_safety\n",
      "Processing folder: mas_safety_gpt4o_bbq_safety\n",
      "Parsed 871 responses from mas_safety_gpt4o_bbq_safety\n",
      "Processing folder: mas_metacognition_gpt4o_metamedqa_metacognition\n",
      "Parsed 1371 responses from mas_metacognition_gpt4o_metamedqa_metacognition\n",
      "Processing folder: mas_metacognition_gpt4o_mmlu_metacognition\n",
      "Parsed 398 responses from mas_metacognition_gpt4o_mmlu_metacognition\n"
     ]
    }
   ],
   "source": [
    "all_responses = {}\n",
    "titan_path = \"/mnt/bulk-titan/sanddhya/agents_edge/Agent_Edge_Benchmarking/out\"\n",
    "for folder in os.listdir(titan_path):\n",
    "    if folder.startswith('mas_safety') or folder.startswith('mas_metacognition'):\n",
    "        print(f'Processing folder: {folder}')\n",
    "        parsed_responses = {}\n",
    "        for f in os.listdir(f'{titan_path}/{folder}/logs'):\n",
    "            if f.endswith('.txt'):\n",
    "                with open(os.path.join(titan_path, folder, 'logs', f), 'r') as file:\n",
    "                    content = file.read()\n",
    "                    vals = content.split('----------------------------------------')\n",
    "                    if len(vals) < 3: \n",
    "                        print(f\"Skipping {f} due to insufficient data.\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        response = vals[-2].split('Content:')[-1].replace('\\n', ' ').replace(' ', '')\n",
    "                        pattern = r'^[0-9],[1-5]$'\n",
    "                        match = re.search(pattern, response)\n",
    "                        if match:\n",
    "                            parsed_responses[f.replace('.txt', '')] = {\n",
    "                                    'answer': int(response.split(',')[0]),\n",
    "                                    'confidence': int(response.split(',')[1])\n",
    "                            }\n",
    "                file.close()\n",
    "        print(f'Parsed {len(parsed_responses)} responses from {folder}')\n",
    "        all_responses[folder] = parsed_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4df2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(results_df, experiment_name):\n",
    "\n",
    "    total = len(results_df)\n",
    "    corrects = results_df['is_correct'].sum()\n",
    "    accuracy = (corrects / total) * 100 if total > 0 else 0\n",
    "\n",
    "    acc_by_conf = results_df.groupby('confidence')['is_correct'].mean() * 100\n",
    "\n",
    "    with open(f\"{experiment_name}_SUMMARY.txt\", 'w') as f:\n",
    "        f.write(f\"Total questions: {total}\\n\")\n",
    "        f.write(f\"Correct: {corrects}\\n\")\n",
    "        f.write(f\"Overall Accuracy: {accuracy:.2f}%\\n\\n\")\n",
    "        f.write(\"Accuracy by confidence level:\\n\")\n",
    "        for conf, acc in acc_by_conf.items():\n",
    "            if pd.notna(conf):\n",
    "                f.write(f\"  {int(conf)}: {acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2d1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate parsed response csv\n",
    "for experiment_name, responses in all_responses.items():\n",
    "    results = []\n",
    "    benchmark_path = f'../../benchmarks/{experiment_name.split(\"_\")[1]}/{experiment_name.split(\"gpt4o_\")[1]}.json'\n",
    "    benchmark_df = pd.DataFrame(json.load(open(benchmark_path, 'r')))\n",
    "    for id, response in responses.items():\n",
    "        answer = int(response['answer'])\n",
    "        confidence = int(response['confidence'])\n",
    "        correct = int(benchmark_df[benchmark_df['id'] == int(id)]['target'].values[0])\n",
    "        is_correct = int(answer == correct)\n",
    "        \n",
    "        results.append({\n",
    "                    'id': int(id),\n",
    "                    'model_answer': answer,\n",
    "                    'confidence': confidence,\n",
    "                    'correct_answer': correct,\n",
    "                    'is_correct': is_correct\n",
    "                    })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    generate_summary(results_df, experiment_name)\n",
    "    results_df.to_csv(f'{experiment_name}_RESPONSES.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66378a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary files for all mas_safety and mas_metacognition\n",
    "# bbq summary for eval optim and response file\n",
    "# bbq summary for zero shot and response file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
