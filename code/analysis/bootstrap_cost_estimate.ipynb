{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36c4c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_GPT_4o_IN = 5 / 1000000\n",
    "OPENAI_GPT_4o_OUT = 20 / 1000000\n",
    "EVAL_OPTIM_FEEDBACK_PROMPT = 80\n",
    "EVAL_OPTIM_FEEDBACK_LEN = 50\n",
    "TOOL_FEEDBACK_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9660d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bf9da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = \"Grade the response as 'valid' or 'not valid' based on whether the response is appropriate for the question and choices provided. \"\n",
    "#s += \"If the response is not valid, provide concise few sentences of feedback (single paragraph) on how to modify the answer and crucial information to consider when answering the question.\"\n",
    "#s += \"If you have provided any previous feedback (listed below), consider it in your evaluation and do not contradict yourself.\\n\"\n",
    "#enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#len(enc.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62f4ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benchmark_stats(benchmark_df):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    avg_in_tokens = benchmark_df[\"question\"].dropna().astype(str).apply(lambda s: len(enc.encode(s))).mean() + benchmark_df[\"options\"].dropna().astype(str).apply(lambda s: len(enc.encode(s))).mean()\n",
    "\n",
    "    avg_out_tokens = 4\n",
    "    num_entries = len(benchmark_df)\n",
    "\n",
    "    return {\n",
    "        \"avg_in_tokens\": avg_in_tokens,\n",
    "        \"avg_out_tokens\": avg_out_tokens,\n",
    "        \"num_entries\": num_entries \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be85719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_file_map = {\n",
    "        'mmlu_ethics' : 'ethics/mmlu_ethics.json',\n",
    "        'triage_ethics' : 'ethics/triage_ethics.json',\n",
    "        'truthfulqa_ethics' : 'ethics/truthfulqa_ethics.json',\n",
    "        'medbullets_metacognition' : 'metacognition/medbullets_metacognition.json',\n",
    "        'medcalc_metacognition' : 'metacognition/medcalc_metacognition.json',\n",
    "        'metamedqa_metacognition' : 'metacognition/metamedqa_metacognition.json',\n",
    "        'mmlu_metacognition' : 'metacognition/mmlu_metacognition.json',\n",
    "        'mmlu_pro_metacognition' : 'metacognition/mmlu_pro_metacognition.json',\n",
    "        'pubmedqa_metacognition' : 'metacognition/pubmedqa_metacognition.json',\n",
    "        'bbq_safety' : 'safety/bbq_safety.json',\n",
    "        'casehold_safety' : 'safety/casehold_safety.json',\n",
    "        'mmlu_safety' : 'safety/mmlu_safety.json',\n",
    "        'mmlupro_safety' : 'safety/mmlupro_safety.json'\n",
    "    }\n",
    "\n",
    "benchmark_len_stats = {}\n",
    "for benchmark in benchmark_file_map:\n",
    "    df = pd.DataFrame(json.load(open(f\"../../benchmarks/{benchmark_file_map[benchmark]}\", 'r'))).set_index('id')\n",
    "    stats = compute_benchmark_stats(df)\n",
    "    benchmark_len_stats[benchmark] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5764a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cost(benchmark_stats, percent_dataset = 0.15):\n",
    "    cost = 0\n",
    "    for _, stats in benchmark_stats.items():\n",
    "        num_entries = int(stats['num_entries'] * percent_dataset)\n",
    "        cost += (stats['avg_in_tokens'] * OPENAI_GPT_4o_IN + stats['avg_out_tokens'] * OPENAI_GPT_4o_OUT) * num_entries\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48b889c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_optimizer_cost(benchmark_stats, percent_dataset = 0.15):\n",
    "    cost = 0\n",
    "    for _, stats in benchmark_stats.items():\n",
    "        num_entries = int(stats['num_entries'] * percent_dataset)\n",
    "        round_1_cost = (((2 * stats['avg_in_tokens'] + EVAL_OPTIM_FEEDBACK_PROMPT) * OPENAI_GPT_4o_IN)) + ((stats['avg_out_tokens'] + EVAL_OPTIM_FEEDBACK_LEN) * OPENAI_GPT_4o_OUT)\n",
    "        round_2_cost = (((2 * stats['avg_in_tokens'] + EVAL_OPTIM_FEEDBACK_PROMPT + (EVAL_OPTIM_FEEDBACK_LEN)) * OPENAI_GPT_4o_IN)) + ((stats['avg_out_tokens'] + EVAL_OPTIM_FEEDBACK_LEN) * OPENAI_GPT_4o_OUT)\n",
    "        round_3_cost = (((2 * stats['avg_in_tokens'] + EVAL_OPTIM_FEEDBACK_PROMPT + (2*EVAL_OPTIM_FEEDBACK_LEN)) * OPENAI_GPT_4o_IN)) + ((stats['avg_out_tokens'] + EVAL_OPTIM_FEEDBACK_LEN) * OPENAI_GPT_4o_OUT)\n",
    "        cost += (num_entries * (round_1_cost + round_2_cost + round_3_cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a616727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mas_cost(benchmark_stats, percent_dataset = 0.15):\n",
    "    cost = 0\n",
    "    for _, stats in benchmark_stats.items():\n",
    "        num_entries = int(stats['num_entries'] * percent_dataset)\n",
    "        num_tool_calls = 4\n",
    "        mas_orchestration_cost = ((stats['avg_in_tokens'] + (4*TOOL_FEEDBACK_LEN)) * OPENAI_GPT_4o_IN) + (stats['avg_out_tokens'] * OPENAI_GPT_4o_OUT) \n",
    "        mas_tool_cost = num_tool_calls * ((stats['avg_in_tokens'] * OPENAI_GPT_4o_IN) + (TOOL_FEEDBACK_LEN * OPENAI_GPT_4o_OUT))\n",
    "        cost += (num_entries * (mas_tool_cost + mas_orchestration_cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b1e896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for 45% of the dataset: $103.66\n"
     ]
    }
   ],
   "source": [
    "total_cost = zero_shot_cost(benchmark_len_stats, 0.45) + eval_optimizer_cost(benchmark_len_stats, 0.45) + mas_cost(benchmark_len_stats, 0.45)\n",
    "print(f\"Total cost for 45% of the dataset: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python boostrapping.py --benchmark mmlu_ethics --n_samples 402 --experiment_path bootstrapping_45percent/mmlu_ethics_N402\n",
      "python boostrapping.py --benchmark triage_ethics --n_samples 38 --experiment_path bootstrapping_45percent/triage_ethics_N38\n",
      "python boostrapping.py --benchmark truthfulqa_ethics --n_samples 355 --experiment_path bootstrapping_45percent/truthfulqa_ethics_N355\n",
      "python boostrapping.py --benchmark medbullets_metacognition --n_samples 138 --experiment_path bootstrapping_45percent/medbullets_metacognition_N138\n",
      "python boostrapping.py --benchmark medcalc_metacognition --n_samples 189 --experiment_path bootstrapping_45percent/medcalc_metacognition_N189\n",
      "python boostrapping.py --benchmark metamedqa_metacognition --n_samples 617 --experiment_path bootstrapping_45percent/metamedqa_metacognition_N617\n",
      "python boostrapping.py --benchmark mmlu_metacognition --n_samples 179 --experiment_path bootstrapping_45percent/mmlu_metacognition_N179\n",
      "python boostrapping.py --benchmark mmlu_pro_metacognition --n_samples 1412 --experiment_path bootstrapping_45percent/mmlu_pro_metacognition_N1412\n",
      "python boostrapping.py --benchmark pubmedqa_metacognition --n_samples 450 --experiment_path bootstrapping_45percent/pubmedqa_metacognition_N450\n",
      "python boostrapping.py --benchmark bbq_safety --n_samples 446 --experiment_path bootstrapping_45percent/bbq_safety_N446\n",
      "python boostrapping.py --benchmark casehold_safety --n_samples 181 --experiment_path bootstrapping_45percent/casehold_safety_N181\n",
      "python boostrapping.py --benchmark mmlu_safety --n_samples 690 --experiment_path bootstrapping_45percent/mmlu_safety_N690\n",
      "python boostrapping.py --benchmark mmlupro_safety --n_samples 495 --experiment_path bootstrapping_45percent/mmlupro_safety_N495\n"
     ]
    }
   ],
   "source": [
    "for benchmark, stats in benchmark_len_stats.items():\n",
    "    print(f\"python boostrapping.py --benchmark {benchmark} --n_samples {int(0.45*stats['num_entries'])} --experiment_path bootstrapping_45percent/{benchmark}_N{int(0.45*stats['num_entries'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c6ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
