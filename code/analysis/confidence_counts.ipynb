{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890053e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "040782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casehold_safety_RESPONSES |\n",
      "{5: 288, 4: 98, 3: 8, 2: 6, 1: 2}\n",
      "medbullets_metacognition_RESPONSES |\n",
      "{5: 274, 4: 28, 3: 4, 1: 2}\n",
      "mmlu_safety_RESPONSES |\n",
      "{5: 1139, 4: 378, 3: 12, 2: 2, 1: 1}\n",
      "mmlupro_safety_RESPONSES |\n",
      "{5: 741, 4: 348, 3: 11, 2: 1}\n",
      "mmlu_metacognition_RESPONSES |\n",
      "{5: 358, 4: 32, 3: 3, 1: 2}\n",
      "mmlu_ethics_RESPONSES |\n",
      "{5: 698, 4: 194, 3: 2, 2: 1}\n",
      "truthfulqa_ethics_RESPONSES |\n",
      "{5: 588, 4: 130, 3: 7, 2: 2, 1: 1}\n",
      "bbq_safety_FILTERED_RESPONSES |\n",
      "{5: 422, 3: 150, 4: 127, 1: 120, 2: 52}\n",
      "medcalc_metacognition_RESPONSES |\n",
      "{5: 323, 4: 46, 3: 9, 2: 8, 1: 5}\n",
      "triage_ethics_RESPONSES |\n",
      "{5: 68, 4: 18}\n",
      "metamedqa_metacognition_RESPONSES |\n",
      "{5: 1192, 4: 153, 3: 17, 1: 6, 2: 5}\n",
      "pubmedqa_metacognition_RESPONSES |\n",
      "{4: 514, 5: 389, 3: 95, 2: 2}\n"
     ]
    }
   ],
   "source": [
    "shared_out_path = '/mnt/bulk-titan/sanddhya/agents_edge/Agent_Edge_Benchmarking/out/gpt4o/evaloptim/'\n",
    "\n",
    "for f in os.listdir(os.path.join(shared_out_path)):\n",
    "    if 'RESPONSES' in f and f.endswith('.csv'):\n",
    "        print(f.replace('.csv',''), '|')\n",
    "        responses_df = pd.read_csv(os.path.join(shared_out_path, f))\n",
    "        print(responses_df['confidence'].value_counts().to_dict())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d51249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
