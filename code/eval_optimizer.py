from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
import pandas as pd
import re
import json 
from tqdm import tqdm
from langchain_core.runnables.config import RunnableConfig
import sys 
from sklearn.model_selection import train_test_split
import os
from IPython.display import Image
from datetime import datetime
from io import TextIOWrapper
from dotenv import load_dotenv

load_dotenv()

class State(TypedDict):
    id: str
    response: int
    confidence: int
    prompt: str
    question: str
    choices: list[str]
    feedback: str
    valid_or_not: str
    log_file: TextIOWrapper

class Feedback(BaseModel):
    grade: Literal["valid", "not valid"] = Field(
        description="Decide if the response chosen is valid for the provided question/choices or not.",
    )
    feedback: str = Field(
        description="If the response is not valid, provide concise, detailed feedback on how to modify the answer and crucial informaiton to consider when answering the question.",
    )

def check_response_format(response: str, num_choices: int) -> bool:
    """
    Check if the response is in the format "ChoiceNumber, ConfidenceScore"
    where ChoiceNumber is an integer and ConfidenceScore is an integer from 1 to 5.
    """
    last_choice_index = num_choices - 1
    regex_string = rf'^\s*[0-{last_choice_index:d}]\s*,\s*[1-5]\s*$'
    pattern = re.compile(regex_string)
    return bool(pattern.match(response))

# Nodes
def llm_call_generator(state: State):
    """LLM generates a response"""
    log_file = state.get("log_file")
    if state.get("feedback"):
        input = f"Consider the following feedback when answering the following question. You do not have to agree with the feedback but you must use it in your reasoning when answering the question:\n\nFEEDBACK:{state['feedback']}\n\nQUESTION:\n\n{state['prompt']}"
    else:
        input = f"{state['prompt']}"
    msg = llm.invoke(input)
    is_valid = check_response_format(msg.content, len(state['choices']))

    llm_name = "llm_call_generator"
    time = datetime.now().isoformat()
    output = f"{msg.content}"
    log_string = f"{time}|\n{llm_name}|\n{input}|\n{output}\n"
    log_file.write(log_string)
    log_file.write(f"******************************\n\n")

    num_attempts = 0
    while not is_valid and num_attempts < 5:
        num_attempts += 1
        msg = llm.invoke(input)
        is_valid = check_response_format(msg.content, len(state['choices']))
        time = datetime.now().isoformat()
        output = f"{msg.content}"
        log_string = f"{time}|\n{llm_name}|\n{input}|\n{output}\n"
        log_file.write(log_string)
        log_file.write(f"******************************\n\n")
    if is_valid:
        response = int(msg.content.split(',')[0].replace(' ','')) 
        confidence = int(msg.content.split(',')[1].replace(' ','')) 
    else:
        response = None
        confidence = None
    return {"response": response, "confidence" : confidence}

def llm_call_evaluator(state: State):
    """LLM evaluates the response"""

    grading_prompt = f"QUESTION:\n{state['question']}"
    i = 0
    for val in state['choices']:
        grading_prompt += f"{i}) {val}\n"
        i += 1
    grading_prompt += f"\nRESPONSE:\n{state['response']}\n\n"
    grading_prompt += "Grade the response as 'valid' or 'not valid' based on whether the response is appropriate for the question and choices provided. "
    grading_prompt += "If the response is not valid, provide concise, detailed feedback on how to modify the answer and crucial information to consider when answering the question."
    grade = evaluator.invoke(grading_prompt)

    log_file = state.get("log_file")
    input = grading_prompt
    llm_name = "llm_call_evaluator"
    time = datetime.now().isoformat()
    output = f"Grade: {grade.grade}, Feedback: {grade.feedback}"
    log_string = f"{time}|\n{llm_name}|\n{input}|\n{output}\n"
    log_file.write(log_string)
    log_file.write(f"******************************\n\n")

    return {"valid_or_not": grade.grade, "feedback": grade.feedback}


def route_response(state: State):
    """Route back to response generator or end based upon feedback from the evaluator"""

    if state["valid_or_not"] == "valid":
        return "Accepted"
    elif state["valid_or_not"] == "not valid":
        return "Rejected + Feedback"

def sample_dataset(df, sample_size=0.1):
    _, sample = train_test_split(
        df,
        test_size=sample_size,
        stratify=df['kind'],
        random_state=42
    )
    return sample

def run_benchmark(dataset, shared_benchmark_path, experiment_name, percent_sample):

    dataset_path = '../../benchmarks/'
    dataset_paths = {
        "mmlu_ethics": "ethics/mmlu_ethics.json",
        "triage_ethics": "ethics/triage_ethics.json",
        "truthfulqa_ethics": "ethics/truthfulqa_ethics.json",
        "metamedqa_metacognition": "metacognition/metamedqa_metacognition.json",
        "medbullets_metacognition": "metacognition/medbullets_metacognition.json",   
        "medcalc_metacognition" : "metacognition/medcalc_metacognition.json",
        "mmlu_metacognition": "metacognition/mmlu_metacognition.json",
        "pubmedqa_metacognition": "metacognition/pubmedqa_metacognition.json",
        "bbq_safety" : "bbq_safety/bbq_safety.json",
        "casehold_safety" : "casehold_safety/casehold_safety.json",
        "mmlupro_safety" : "safety/mmlupro_safety.json",
        "mmlu_safety" : "safety/mmlu_safety.json"
    }

    ds = json.load(open(f"{dataset_path}/{dataset_paths[dataset]}", 'r'))
    df = pd.DataFrame(ds)
    df_subset = sample_dataset(df, sample_size=percent_sample) 
    
    results = []
    for idx, row in tqdm(df_subset .iterrows(), total=len(df_subset), desc=f"{experiment_name} - {dataset} ({percent_sample*100:.1f}%)"):

        f = open(f'{shared_benchmark_path}/{experiment_name}_{dataset}/logs/{str(row["id"])}.txt', 'w')
        f.write("TIME | LLM | INPUT | OUTPUT\n")
        f.write(f"******************************\n\n")
        id = row['id']
        question = row['question']
        choices = row['options']
        correct = row['target']
        
        choice_string = ""
        i = 0
        i_string = ""
        for val in choices:
            choice_string += f"{i}) {val}\n"
            i_string += f", {i}"
            i += 1

        prompt = (
            f"Question: {question}\n"
            f"Choices:\n{choice_string}\n"
            "Please answer with only two values separated by a comma: "
            "'ChoiceNumber, ConfidenceScore' where ChoiceNumber is the index (zero-based) of your answer "
            f"(e.g.{i_string}) and ConfidenceScore is an integer from 1 to 5."
        )
        

        state = optimizer_workflow.invoke({"question": question, "prompt" : prompt,
                                   "choices": choices, 
                                   "log_file": f, "id":id}, RunnableConfig(recursion_limit=100))
        answer = state["response"]
        confidence = state["confidence"]

        if answer is not None and confidence is not None:
        
            is_correct = int(answer == correct)
        
            results.append({
                    'id': id,
                    'model_answer': answer,
                    'confidence': confidence,
                    'correct_answer': correct,
                    'is_correct': is_correct
                    })
            
        f.close()

    results_df = pd.DataFrame(results)
    results_df.to_csv(f'{shared_benchmark_path}/{experiment_name}_{dataset}/RESPONSES.csv', index=False)

    return results_df

def setup_experiment_directory(experiment_name, dataset_name, shared_benchmark_path, percent_sample):
    path = f'{shared_benchmark_path}/{experiment_name}_{dataset_name}'
    if os.path.exists(path):
        # Directory (or file) already there â†’ error out
        sys.exit(f"Error: '{path}' already exists. Aborting.")
    try:
        os.makedirs(path, exist_ok=False)
        os.makedirs(f'{path}/logs', exist_ok=False)
        with open(f"{path}/INFO.txt", "w") as info_file:
            info_file.write(datetime.now().isoformat())
            info_file.write(f"\nExperiment: {experiment_name}\n")
            info_file.write(f"Dataset: {dataset_name}\n")
            info_file.write(f"Percent Sample: {str(percent_sample)}\n")
            info_file.write(f"Shared Benchmark Path: {shared_benchmark_path}\n")
    except Exception as e:
        sys.exit(f"Error creating '{path}': {e}")

def generate_summary(results_df, shared_benchmark_path, experiment_name, dataset_name):

    total = len(results_df)
    corrects = results_df['is_correct'].sum()
    accuracy = (corrects / total) * 100 if total > 0 else 0

    acc_by_conf = results_df.groupby('confidence')['is_correct'].mean() * 100

    with open(f"{shared_benchmark_path}/{experiment_name}_{dataset_name}/SUMMARY.txt", 'w') as f:
        f.write(f"Total questions: {total}\n")
        f.write(f"Correct: {corrects}\n")
        f.write(f"Overall Accuracy: {accuracy:.2f}%\n\n")
        f.write("Accuracy by confidence level:\n")
        for conf, acc in acc_by_conf.items():
            if pd.notna(conf):
                f.write(f"  {int(conf)}: {acc:.2f}%\n")

def generate_workflow():
    optimizer_builder = StateGraph(State)
    optimizer_builder.add_node("llm_call_generator", llm_call_generator)
    optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)
    optimizer_builder.add_edge(START, "llm_call_generator")
    optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
    optimizer_builder.add_conditional_edges(
        "llm_call_evaluator",
        route_response,
        {  
            "Accepted": END,
            "Rejected + Feedback": "llm_call_generator",
        },
    )

    return optimizer_builder


if __name__ == "__main__":
    model_name = sys.argv[1] if len(sys.argv) > 1 else 'gpt-4o-2024-08-06'
    dataset = sys.argv[2] 
    percent_sample = float(sys.argv[3])
    experiment_name = sys.argv[4] if len(sys.argv) > 2 else 'evaloptimizer_no_memory'
    shared_benchmark_path = sys.argv[5] if len(sys.argv) > 3 else '/Users/sanddhyajayabalan/Desktop/Projects/Prj_MetaMedQA/experiments/v3'

    setup_experiment_directory(experiment_name=experiment_name, 
                                dataset_name=dataset, 
                                shared_benchmark_path=shared_benchmark_path, 
                                percent_sample=percent_sample)

    llm = ChatOpenAI(model=model_name, api_key=os.getenv("OPENAI_API_KEY"))
    evaluator = llm.with_structured_output(Feedback)
    optimizer_workflow = generate_workflow().compile()
    img = Image(optimizer_workflow.get_graph().draw_mermaid_png())
    with open(f'{shared_benchmark_path}/{experiment_name}_{dataset}/WORKFLOW.png', "wb") as f:
        f.write(img.data)

    results = run_benchmark(
            dataset=dataset,
            shared_benchmark_path=shared_benchmark_path,
            experiment_name=experiment_name,
            percent_sample=percent_sample
        )

    generate_summary(
            results_df=results,
            shared_benchmark_path=shared_benchmark_path,
            experiment_name=experiment_name,
            dataset_name=dataset
        )


